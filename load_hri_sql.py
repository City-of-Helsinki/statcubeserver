# encoding: utf-8
import os
import sys
from urllib2 import urlopen
import urllib2
import urlparse
import json
import pydatacube
import pydatacube.pcaxis
import pydatacube.sql
import psycopg2


METADATA_URL="http://www.hri.fi/wp-content/uploads/ckan/hri-ckan-active-metadata-daily-output.json"
PXLIST_URL="http://www.hel.fi/hel2/tietokeskus/data/ymparistotilasto/index.txt"

# From the werkzeug-project: https://github.com/mitsuhiko/werkzeug/blob/master/werkzeug/urls.py
import urllib
def url_fix(s, charset='utf-8'):
    """Sometimes you get an URL by a user that just isn't a real
    URL because it contains unsafe characters like ' ' and so on.  This
    function can fix some of the problems in a similar way browsers
    handle data entered by the user:

    >>> url_fix(u'http://de.wikipedia.org/wiki/Elf (BegriffsklÃ¤rung)')
    'http://de.wikipedia.org/wiki/Elf%20%28Begriffskl%C3%A4rung%29'

    :param charset: The target charset for the URL if the url was
                    given as unicode string.
    """
    if isinstance(s, unicode):
        s = s.encode(charset, 'ignore')
    scheme, netloc, path, qs, anchor = urlparse.urlsplit(s)
    path = urllib.quote(path, '/%')
    qs = urllib.quote_plus(qs, ':&=')
    return urlparse.urlunsplit((scheme, netloc, path, qs, anchor))

#=======these are used to import the packages announced by CKAN=========
def iterate_resources(packages):
    for package in packages:
        for resource in package['resources']:
            if resource['format'] != 'pc-axis':
                continue
            url = url_fix(resource['url'])
            yield dict(package=package, url=url, format='pc-axis')

def load_metadata(url):
    metadata = json.load(urlopen(url))
    packages = metadata['packages']
    return list(iterate_resources(packages))
#======================================================================

#=======these are almost the same, but are are used to import files from a txt file which lists the urls of the px files ===========
def iterate_files(filelist):
    for f in filelist:
        retdict = {}
        retdict['url'] = url_fix(f)
        retdict['format'] = 'pc-axis'
        retdict['package'] = {
            'name': f.split('/')[-1].split('.')[0],
            'ckan_url': 'http://no-ckan-urls-here.com',
            'format': 'pc-axis',
            'resources': []
            }
        yield retdict

def load_filelist_data(url):
    filelist = urlopen(url)
    files = filelist.readlines()
    files = [f.strip('\n') for f in files]
    return list(iterate_files(files))
#==========================================

def load_resources(db_connection, replace=False):
    # resources = load_metadata(METADATA_URL)
    resources = load_filelist_data(PXLIST_URL)

    def connect():
        return psycopg2.connect(db_connection)

    con = connect()
    pydatacube.sql.initialize_schema(con)
    con.commit()
    for resource in resources:
        try:
            id = resource['package']['name']
            print >>sys.stderr, "Loading %s from %s"%(id, resource['url'])

            con = connect()
            if not replace:
                if pydatacube.sql.SqlDataCube.Exists(con, id):
                    continue
            data = urlopen(resource['url'])
            try:
                cube = pydatacube.pcaxis.to_cube(data,
                    origin_url=resource['url'])
            except pydatacube.pcaxis.PxSyntaxError, e:
                print >>sys.stderr, "Px parsing failed:", e
                continue
            try:
                pydatacube.sql.SqlDataCube.FromCube(con, id, cube, replace=True)
                con.commit()
            finally:
                con.close()
        except urllib2.HTTPError, e:
            print >>sys.stderr, "Download error", e, resource['package']['ckan_url']
            try:
                from urlparse import urlparse, parse_qs, urlunparse
                alternate = (r for r in resource['package']['resources'] if r['format'] == 'tietokanta').next()
                urlparts = urlparse(alternate['url'])
                url = parse_qs(urlparts.query)['bmark'][0].lower() + ".px"
                url = '/'.join(url.split('/')[1:])
                # WTF Guido, why is the URL API so damn horrible?
                url = urlunparse(list(urlparts[:2]) + [url] + ['']*3)
                print >>sys.stderr, "The right URL is probably %s"%(url,)
            except StopIteration:
                print >>sys.stderr, "'Tietokanta' link missing, can't guess the right url"
            print >>sys.stderr, ""

if __name__ == '__main__':
    import argh
    argh.dispatch_command(load_resources)
